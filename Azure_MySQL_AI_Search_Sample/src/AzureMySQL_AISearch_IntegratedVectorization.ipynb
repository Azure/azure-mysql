{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of sample\n",
    "\n",
    "We'll create a small example of an AI application that responds to users' queries based on a MySQL table of product reviews. \n",
    "\n",
    "At the end of the example, the solution will look something like this:\n",
    "\n",
    "```\n",
    "[User search]: Light weight gym bag\n",
    "[AI Response]: After searching through our product database, I recommend <product ID> because... \n",
    "```\n",
    "\n",
    "Behind the scenes, we take the following steps:\n",
    "* Prep work: Set up a sample table in a Azure MySQL Flexible Server DB and upload data to it\n",
    "* Step 1 - Set up data source connection in Azure AI Search \n",
    "* Step 2 - Set up automatic chunking + vectorization + indexing \n",
    "  * Set up an index in Azure AI Search to store the data we need, including vectorized versions of the text reviews.\n",
    "  * Set up an indexer in Azure AI Search to pull data into the index \n",
    "  * Add a skillset to automatically chunk and vectorize the data using an Azure OpenAI Embedding service\n",
    "* Step 3 - Use vector search from a sample application \n",
    "* Step 4 - Generate a GPT response to the user \n",
    "  * Create an Azure OpenAI Chat Completion service to utilize the Azure AI Search as datasource.\n",
    "  * Use the Azure OpenAI Chat Completion service to respond to the user's query \n",
    "\n",
    "Copyright (c) Microsoft Corporation.\n",
    "Licensed under the MIT license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "You will need:\n",
    "* An existing Azure Database for MySQL Flexible Server with server name, DB name, username, and password copied into `example.env`\n",
    "  * The user must have permission to create a new table\n",
    "  * You must whitelist your IP to access your Azure Database for MySQL flexible server by opening the MySQL server resource in the Azure portal, navigating to Security / Networking, and adding your IP.\n",
    "* An OpenAI resource with the endpoint and key copied into `example.env`\n",
    "* An Azure AI Search resource with the endpoint and key copied into `example.env`\n",
    "* The Python packages listed in `requirements.txt` (can be installed using `pip`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environment variables and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "# specify the name of the .env file name \n",
    "env_name = \"./src/example.env\"\n",
    "config = dotenv_values(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure database for MySQL flexible server connection details\n",
    "server = config[\"server\"]\n",
    "database = config[\"database\"]\n",
    "username = config[\"username\"]\n",
    "password = config[\"password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Open AI deployment details\n",
    "import openai\n",
    "openai.api_type = config[\"openai_api_type\"]\n",
    "openai.api_key = config['openai_api_key']\n",
    "openai.api_base = config['openai_api_base']\n",
    "openai.api_version = config['openai_api_version'] \n",
    "openai_deployment_embedding = config[\"openai_deployment_embedding\"]\n",
    "openai_deployment_completion = config[\"openai_deployment_completion\"]\n",
    "EMBEDDING_LENGTH = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cognitive Search service details\n",
    "azureai_search_key = config[\"azureai_search_api_key\"]\n",
    "service_endpoint = config[\"azureai_search_endpoint\"]\n",
    "index_name = config[\"azureai_search_index_name\"] # Desired name of index -- does not need to exist already\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data to MySQL DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to database\n",
    "\n",
    "For simplicity, we set `autocommit=True` in the mysql connector parameters, which allows us to execute `ALTER` statements. \n",
    "\n",
    "If a timeout error occurs, retry the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Establish a connection to the Azure MySQL database\n",
    "conn = mysql.connector.connect(host=server, user=username, password=password, database=database, autocommit=True, ssl_disabled=False, ssl_verify_cert=False, ssl_verify_identity=False)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table in the database\n",
    "\n",
    "We will create a new table \"foodreview\" and upload the data from a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"productreviews\" \n",
    "\n",
    "# Drop previous table of same name if one exists\n",
    "cursor.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "print(\"Finished dropping table (if existed)\")\n",
    "\n",
    "# Create a table\n",
    "cursor.execute(f\"\"\"\n",
    "               CREATE TABLE {table_name} \n",
    "               (Id integer NOT NULL, \n",
    "               ProductId text, \n",
    "               ProductName text, \n",
    "               ProductImage text, \n",
    "               ProductUrl text, \n",
    "               ReviewId text, \n",
    "               Summary text, \n",
    "               Text text,\n",
    "               ReviewerNickName text,\n",
    "               TextConcat text,\n",
    "               Created DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "               LastUpdated DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "               Deleted integer DEFAULT 0,\n",
    "               PRIMARY KEY (Id)\n",
    "               );\n",
    "               \"\"\")\n",
    "print(\"Finished creating table\")\n",
    "\n",
    "\n",
    "server_change_detection_column_name = \"LastUpdated\"\n",
    "server_soft_delete_column_name = \"Deleted\"\n",
    "server_soft_delete_marker_value = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from CSV\n",
    "\n",
    "The data contains a few product reviews, with related info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_all = pd.read_csv('./DataSet/magento_reviews.csv')\n",
    "\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate data\n",
    "\n",
    "For our example, we will combine the user's summary with the user's review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"TextConcat\"] = df_all.apply(lambda row: f\"Summary: {row['Summary']} | Review: {row['Text']}\",\n",
    "                                    axis = 1)\n",
    "\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into batches\n",
    "batch_size = 30\n",
    "batches = [df_all[i:i + batch_size] for i in range(0, len(df_all), batch_size)]\n",
    "\n",
    "#Iterate over each batch and insert the data into the database\n",
    "for batch in batches:\n",
    "    # Convert the batch dataframe to a list of tuples for bulk insertion\n",
    "    rows = [tuple(row) for row in batch.itertuples(index=False)]\n",
    "    \n",
    "    # Define the SQL query for bulk insertion\n",
    "    query = f\"INSERT INTO {table_name} (Id, ProductId, ProductName, ProductImage, ProductUrl, ReviewId, Summary, Text, ReviewerNickName, TextConcat) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "    \n",
    "    cursor.executemany(query, rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example query\n",
    "\n",
    "This checks that the data was uploaded correctly. We should have 99 rows at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the SELECT statement\n",
    "try:\n",
    "    cursor.execute(f\"SELECT count(Id) FROM {table_name};\")\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(f\"Error executing SELECT statement: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Set up data source connection in Azure AI Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed Azure AI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient  \n",
    "from azure.search.documents.models import (\n",
    "    QueryAnswerType,\n",
    "    QueryCaptionType,\n",
    "    QueryLanguage,\n",
    "    QueryType,\n",
    "    VectorizableTextQuery,\n",
    "    VectorFilterMode,    \n",
    ")\n",
    "from azure.search.documents.indexes.models import (   \n",
    "    AzureOpenAIParameters,  \n",
    "    AzureOpenAIVectorizer,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    SemanticPrioritizedFields,   \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SearchIndex,   \n",
    "    SearchIndexerDataContainer,  \n",
    "    SemanticConfiguration,  \n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    VectorSearch,  \n",
    "    VectorSearchAlgorithmMetric,  \n",
    "    VectorSearchProfile,\n",
    "    SplitSkill,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    SearchIndexerIndexProjections,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerSkillset,\n",
    "    HighWaterMarkChangeDetectionPolicy,\n",
    "    SoftDeleteColumnDeletionDetectionPolicy,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexer\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data source connection\n",
    "\n",
    "This step creates a connection that will be used to pull data from our MySQL table.\n",
    "\n",
    "Documentation can be found [here.](https://learn.microsoft.com/en-us/azure/search/search-howto-index-mysql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_conn_str = f'Server={server}; Port=3306; Database={database}; Uid={username}; Pwd={password}; SslMode=Preferred;'\n",
    "\n",
    "azureai_search_credential = AzureKeyCredential(azureai_search_key)\n",
    "ds_client = SearchIndexerClient(service_endpoint, azureai_search_credential)\n",
    "container = SearchIndexerDataContainer(name=table_name)\n",
    "\n",
    "change_detection_policy = HighWaterMarkChangeDetectionPolicy(high_water_mark_column_name=server_change_detection_column_name)\n",
    "\n",
    "soft_delete_detection_policy = SoftDeleteColumnDeletionDetectionPolicy(\n",
    "    soft_delete_column_name=server_soft_delete_column_name,\n",
    "    soft_delete_marker_value=server_soft_delete_marker_value)\n",
    "\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=f\"{index_name}-mysql-connection\",\n",
    "    type=\"mysql\",\n",
    "    connection_string=ds_conn_str,\n",
    "    container=container,\n",
    "    data_change_detection_policy=change_detection_policy,\n",
    "    data_deletion_detection_policy=soft_delete_detection_policy\n",
    ")\n",
    "\n",
    "data_source = ds_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Set up automatic chunking, vectorization and indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create index\n",
    "\n",
    "The plan is:\n",
    "1. Take the combined text (summary + review text) from each product review\n",
    "2. Split the combined text into chunks\n",
    "3. Embed each chunk as a vector\n",
    "4. (Later) search for the most relevant chunk based on the incoming query. \n",
    "\n",
    "To enable this, the search index will store all of the following data, for each chunk of text:\n",
    "* Id of chunk\n",
    "* Chunk text\n",
    "* Vector version of chunk text\n",
    "* Id of parent row\n",
    "* Product Id from parent row\n",
    "* Review text from parent row\n",
    "* Summary text from parent row\n",
    "* Score from parent row\n",
    "\n",
    "All of these values will be stored in SearchFields specified in the code below.\n",
    "\n",
    "In this step we also configure the search algorithm(s), and the vectorizer that will automatically vectorize the incoming query.\n",
    "\n",
    "Documentation about creating indexes can be found [here.](https://learn.microsoft.com/en-us/azure/search/search-how-to-create-search-index?tabs=index-other-sdks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=azureai_search_credential)\n",
    "\n",
    "fields = [\n",
    "    # Properties of individual chunk\n",
    "    SearchField(name=\"Id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),\n",
    "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),\n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), vector_search_dimensions=EMBEDDING_LENGTH, vector_search_profile_name=\"my-vector-search-profile\"),\n",
    "    # Properties of original row in DB that the chunk belonged to\n",
    "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"parent_product_id\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"parent_product_name\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"parent_product_url\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"parent_text\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"parent_summary\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"my-hnsw-config\",\n",
    "            parameters = HnswParameters( \n",
    "                m=4,  \n",
    "                ef_construction=400,  \n",
    "                ef_search=500,  \n",
    "                metric=VectorSearchAlgorithmMetric.COSINE,  \n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"my-vector-search-profile\",\n",
    "            algorithm_configuration_name=\"my-hnsw-config\",\n",
    "            vectorizer=\"my-openai\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            name=\"my-openai\",\n",
    "            kind=\"azureOpenAI\",\n",
    "            azure_open_ai_parameters=AzureOpenAIParameters(\n",
    "                resource_uri=openai.api_base,\n",
    "                deployment_id=openai_deployment_embedding,\n",
    "                api_key=openai.api_key\n",
    "            )\n",
    "        )  \n",
    "    ]  \n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        content_fields=[SemanticField(field_name=\"chunk\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic search with the configuration  \n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f'{result.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create skillset\n",
    "\n",
    "We use two pre-built skills:\n",
    "1. The Split Skill takes the concatenated text and divides it into chunks (to stay within the token limits for the OpenAI embedding service).\n",
    "2. The Azure Open AI Embedding Skill takes the outputs of the Split Skill and vectorizes them individually.\n",
    "\n",
    "Afterwards, we apply an Index Projector to make it so that our final index has one item for every chunk of text (rather than one item for every original row in the DB).\n",
    "\n",
    "We recommend the following resources to learn more about the process and how one can adapt it to different applications:\n",
    "* [Overview of indexers](https://learn.microsoft.com/en-us/azure/search/search-indexer-overview)\n",
    "* [Skill context and input annotation language](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-annotation-language)\n",
    "* [Reference inputs and outputs in skillsets](https://learn.microsoft.com/en-us/azure/search/cognitive-search-concept-annotations-syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a skillset  \n",
    "skillset_name = f\"{index_name}-skillset\"\n",
    "\n",
    "split_skill = SplitSkill(  \n",
    "    description=\"Split skill to chunk documents\",  \n",
    "    text_split_mode=\"pages\",  \n",
    "    context=\"/document\",  \n",
    "    maximum_page_length=300,  \n",
    "    page_overlap_length=20,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/TextConcat\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")  \n",
    "    ]  \n",
    ")\n",
    "\n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
    "    description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
    "    context=\"/document/pages/*\",  \n",
    "    resource_uri=openai.api_base,  \n",
    "    deployment_id=openai_deployment_embedding,  \n",
    "    api_key=openai.api_key,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"vector\")  \n",
    "    ]  \n",
    ")  \n",
    "\n",
    "index_projections = SearchIndexerIndexProjections(  \n",
    "    selectors=[  \n",
    "        SearchIndexerIndexProjectionSelector(  \n",
    "            target_index_name=index_name,  \n",
    "            parent_key_field_name=\"parent_id\", # Note: this populates the \"parent_id\" search field\n",
    "            source_context=\"/document/pages/*\",  \n",
    "            mappings=[  \n",
    "                InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),\n",
    "                InputFieldMappingEntry(name=\"vector\", source=\"/document/pages/*/vector\"),\n",
    "                InputFieldMappingEntry(name=\"parent_product_id\", source=\"/document/ProductId\"),\n",
    "                InputFieldMappingEntry(name=\"parent_product_name\", source=\"/document/ProductName\"),\n",
    "                InputFieldMappingEntry(name=\"parent_product_url\", source=\"/document/ProductUrl\"),\n",
    "                InputFieldMappingEntry(name=\"parent_text\", source=\"/document/Text\"),\n",
    "                InputFieldMappingEntry(name=\"parent_summary\", source=\"/document/Summary\")\n",
    "            ],  \n",
    "        ),  \n",
    "    ],\n",
    ")  \n",
    "\n",
    "skillset = SearchIndexerSkillset(  \n",
    "    name=skillset_name,  \n",
    "    description=\"Skillset to chunk documents and generating embeddings\",  \n",
    "    skills=[split_skill, embedding_skill],\n",
    "    index_projections=index_projections  \n",
    ")\n",
    "  \n",
    "client = SearchIndexerClient(service_endpoint, azureai_search_credential)  \n",
    "client.create_or_update_skillset(skillset)  \n",
    "print(f' {skillset.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexer  \n",
    "indexer_name = f\"{index_name}-indexer\"  \n",
    "\n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to chunk documents and generate embeddings\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=index_name,  \n",
    "    data_source_name=data_source.name\n",
    ")  \n",
    "  \n",
    "indexer_client = SearchIndexerClient(service_endpoint, azureai_search_credential)\n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "\n",
    "# Run the indexer  \n",
    "indexer_client.run_indexer(indexer_name)\n",
    "print(f' {indexer_name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the status of the indexer  \n",
    "indexer_status = indexer_client.get_indexer_status(indexer_name)\n",
    "print(f\"Indexer status: {indexer_status.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow some time for the indexer to process the data\n",
    "import time\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Use vector search from a sample application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Light weight gym bag\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following output, we find the top 3 chunks that are most relevant to the user's query.\n",
    "\n",
    "Feel free to retry the following cell in case of an empty response or a 429 error. An empty response probably indicates that the chunking/embedding process has not finished yet. A 429 error means there have been too many requests to the OpenAI embedding service and should go away on retrying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(service_endpoint, index_name, credential=azureai_search_credential)\n",
    "vector_query = VectorizableTextQuery(text=user_query, k_nearest_neighbors=3, fields=\"vector\", exhaustive=True)\n",
    "  \n",
    "results = search_client.search(\n",
    "    search_text=user_query,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"Id\", \"parent_id\", \"chunk\", \"parent_product_id\", \"parent_product_name\", \"parent_product_url\", \"parent_text\", \"parent_summary\"],\n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    top=5\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Search score: {result['@search.score']}\")\n",
    "    print(f\"Search re-ranker score: {result['@search.reranker_score']}\")\n",
    "    print(f\"Parent Id: {result['parent_id']} | Chunk id: {result['Id']}\")\n",
    "    print(f\"Product Name: {result['parent_product_name']}\")\n",
    "    print(f\"Product Url: {result['parent_product_url']}\")\n",
    "    print(f\"Text chunk: {result['chunk']}\") \n",
    "    print(f\"Review summary: {result['parent_summary']}\")\n",
    "    print(f\"Review text: {result['parent_text']}\")\n",
    "    print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Generate a GPT Response to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt for AI chat completion \n",
    "system_prompt = \"\"\"\n",
    "    You are an AI assistant that recommends product to people based on the product reviews data matching their query. \n",
    "    Your answer should summarize the review text, include the product ID, include the parent id as review id, and mention the overall sentiment of the review.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    base_url=f\"{openai.api_base}/openai/deployments/{openai_deployment_completion}/extensions\",\n",
    "    api_key=openai.api_key,\n",
    "    api_version=openai.api_version,\n",
    ")\n",
    "\n",
    "message_text = [{\"role\": \"user\", \"content\": user_query}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=message_text,\n",
    "    model=openai_deployment_completion,\n",
    "    extra_body={\n",
    "        \"dataSources\":[\n",
    "            {\n",
    "                \"type\": \"AzureCognitiveSearch\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": service_endpoint,\n",
    "                    \"indexName\": index_name,\n",
    "                    \"semanticConfiguration\": \"my-semantic-config\",\n",
    "                    \"queryType\": \"vectorSemanticHybrid\",\n",
    "                    \"inScope\": True,\n",
    "                    \"roleInformation\": system_prompt,\n",
    "                    \"strictness\": 3,\n",
    "                    \"topNDocuments\": 5,\n",
    "                    \"key\": azureai_search_key,\n",
    "                    \"embeddingDeploymentName\": openai_deployment_embedding\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    n=3,\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(completion.model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
